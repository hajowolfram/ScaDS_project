Initialize actor network μ(s|θ^μ) and critic network Q(s, a|θ^Q) with weights θ^μ and θ^Q
Initialize target networks μ'(s|θ^μ') and Q'(s, a|θ^Q') with weights θ^μ' ← θ^μ, θ^Q' ← θ^Q
Initialize replay buffer R

for each episode do:
    Initialize a random process N for action exploration
    Receive initial observation state s1
    for t = 1 to T do:
        Select action a_t = μ(s_t|θ^μ) + N_t according to the current policy and exploration noise
        Execute action a_t and observe reward r_t and new state s_{t+1}
        Store transition (s_t, a_t, r_t, s_{t+1}) in R
        Sample a random minibatch of N transitions (s_i, a_i, r_i, s_{i+1}) from R
        
        Set y_i = r_i + γ Q'(s_{i+1}, μ'(s_{i+1}|θ^μ')|θ^Q')   // target critic value
        Update critic by minimizing the loss: L = 1/N Σ (y_i - Q(s_i, a_i|θ^Q))^2
        Update the actor policy using the sampled policy gradient:
            ∇_θ^μ J ≈ 1/N Σ ∇_a Q(s, a|θ^Q)|_s=s_i,a=μ(s_i) ∇_θ^μ μ(s|θ^μ)|_s_i
        Update the target networks:
            θ^Q' ← τθ^Q + (1 - τ)θ^Q'
            θ^μ' ← τθ^μ + (1 - τ)θ^μ'
    end for
end for
